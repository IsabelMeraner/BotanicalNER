# usr/bin/env python3
# author: Isabel Meraner
# Project: Neural Entity Recognition for Scientific and Vernacular Plant Names (MA-Thesis)
# Institute of Computational Linguistics (University of Zurich), 2019

"""
Evaluate model performance in cross-corpus setting:
Use tagged output from script tagger.py (c.f. Lample et al. 2016) and compare against silver or gold standard for evaluation.

How to run the code:
$ python3 cross_dataset_evaluation.py -s ./../../../02_Evaluation/cross_corpus_evaluation/silver_standard_fold1/plantblog_corpus.test.fold1.txt
    -t ./../../../02_Evaluation/cross_corpus_evaluation/tagged_data/model_wiki_test_blog_f1_fasttext_chardim25_dropout5.tsv

"""

from collections import defaultdict
import argparse
import sys


def get_tagged(tagged):
    """
    Read in tagged file with predicted entity candidates.
    :param tagged: file-like object
    :return: list_tagged (list) containing predicted tokens and labels
    """
    list_tagged = []
    for line in tagged:
        tagged_tokens = line.rstrip("\n").split(" ")
        for tagged_token in tagged_tokens:
            try:
                token, tag = tagged_token.split("__")
            except ValueError:
                print("VALUE ERROR no splitting possible at __", line, file=sys.stderr, flush=True)

            list_tagged.append((token, tag))
    return list_tagged

def get_silver(silver):
    """
    Read in silver (or gold) standard with ground truth labels.
    :param silver: file-like object
    :return: list_silver (list) containing silver tokens and labels
    """
    list_silver = []
    for line in silver:
        try:
            token, *rest, tag = line.rstrip("\n").split("\t")
            list_silver.append((token, tag))
        except:
            continue

    return list_silver

def compute_scores(list_tagged, list_silver):
    """
    Compute evaluation metrics precision, recall, f-score. 
    :param list_tagged: list of tokens and model predictions
    :param list_silver: list of silver standard tokens and annotations
    :return: acc, prec, rec, f1, tokens_counter, check_set_classes
    """
    tp = 0
    non_matching_tags = 0
    fn = 0
    fp_from_tag_to_0 = 0
    fp_from_tag1_to_tag2 = 0
    tn = 0

    matching_dict = defaultdict(int)

    tokens_counter = 0
    check_set_classes = set()

    for line_tagged, line_silver in zip(list_tagged, list_silver):
        token_tagged, iob_tagged = line_tagged
        token_silver, *tags, iob_silver = line_silver
        check_set_classes.add(iob_tagged)
        check_set_classes.add(iob_silver)

        tokens_counter += 1

        # check for missing tags in files
        if iob_tagged == "" or iob_silver == "":
            raise ValueError("IOB-tags should not be empty in line: {}".format(line_tagged, line_silver))

        if iob_tagged == 'O' and iob_silver == 'O':
            tn += 1

        if iob_tagged != 'O' and iob_silver != 'O':
            if iob_tagged == iob_silver:
                tp += 1
                matching_dict[iob_tagged] += 1
            else:
                fp_from_tag1_to_tag2 += 1
                non_matching_tags += 1

        else:
            if iob_tagged == "O" and iob_silver != "O":
                fn += 1
                non_matching_tags += 1
            elif iob_tagged != "O" and iob_silver == "O":
                fp_from_tag_to_0 += 1
                non_matching_tags += 1

    #perc_matching = (tp / tokens_counter) * 100
    #per_nonmatching = (non_matching_tags / tokens_counter) * 100

    acc = (tp + tn) / (tp + tn + fp_from_tag1_to_tag2 + fp_from_tag_to_0 + fn) * 100
    prec = tp / (tp + fp_from_tag_to_0 + fp_from_tag1_to_tag2) * 100
    rec = tp / (tp + fn) * 100
    f1 = ((2 * tp) / ((2 * tp) + (fp_from_tag_to_0 + fp_from_tag1_to_tag2) + fn)) * 100

    return acc, prec, rec, f1, tokens_counter, check_set_classes

def main():
    testfile = "./../../resources/corpora/gold_standard/de/alldata.test.fold1GOLD_de.txt"
    tagged_file = "./../../resources/corpora/de/model_en_alldata_test_alldataGOLD_f1_fasttext_chardim50_dropout5.tsv"

    parser = argparse.ArgumentParser(
        description='Evaluate tagging performance in cross-corpus evaluation setting.')

    parser.add_argument(
        '-s', '--silver_file',
        type=str,
        default=testfile,
        help='test file (silver | gold) in CoNNL-2003 format with IOB annotations')

    parser.add_argument(
        '-t', '--tagged_file',
        type=str,
        default=tagged_file,
        help='tagged output generated by tagger.py')

    args = parser.parse_args()
    test_file = args.silver_file
    tagged_file = args.tagged_file

    with open(tagged_file, 'r') as tagged, open(test_file, 'r') as silver:
        list_tagged = get_tagged(tagged)
        #print("Length of tagged file list: {}".format(len(list_tagged)))
        list_silver = get_silver(silver)
        #print("Length of silver file list: {}".format(len(list_silver)))

        acc, prec, rec, f1, tokens_counter, check_set_classes = compute_scores(list_tagged, list_silver)

        print(20 * "-", file=sys.stderr, flush=True)
        print("Evaluation for model: {}".format(tagged_file), file=sys.stderr, flush=True)
        print("Found {} lines".format(tokens_counter), file=sys.stderr, flush=True)
        print("Found {} unique classes of IOB-tags:\n{}".format(len(check_set_classes), check_set_classes), file=sys.stderr, flush=True)
        print(20 * "-", file=sys.stderr, flush=True)
        print("Accuracy: {:.2f}%\nPrecision: {:.2f}%\nRecall: {:.2f}%\nF1: {:.2f}%".format(acc, prec, rec, f1), file=sys.stderr, flush=True)
        print(20 * "-", file=sys.stderr, flush=True)


if __name__ == '__main__':
    main()
